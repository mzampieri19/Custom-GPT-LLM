{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86a8f26",
   "metadata": {},
   "source": [
    "# Pre Training Custom GPT LLM\n",
    "\n",
    "## Author: Michelangelo Zampieri\n",
    "\n",
    "This notebook contains code to build a custom gpt LLM. \n",
    "\n",
    "The code was generated following the youtube tutorial \"Create a Large Language Model from Scratch with Python â€“ Tutorial\" by freeCodeCamp.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23b07d",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea6cff2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ipykernel.kernelapp' has no attribute 'launch_new_instance'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7135988",
   "metadata": {},
   "source": [
    "Define hyper paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9c09364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 32\n",
    "batch_size = 32\n",
    "max_iters = 50000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 100\n",
    "n_embd = 128\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc81c7c",
   "metadata": {},
   "source": [
    "Read the vocab text and create a sorted array of chars and get its size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc3a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('train_data.txt', 'r', encoding='utf-8') as f1, \\\n",
    "#      open('val_data.txt', 'r', encoding='utf-8') as f2:\n",
    "#     text = f1.read() + f2.read()\n",
    "# chars = sorted(list(set(text)))\n",
    "# vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "193d0acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelangelozampieri/Desktop/gpt-course/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:799: UserWarning: Not enough free disk space to download the file. The expected file size is: 2152.32 MB. The target location /Users/michelangelozampieri/.cache/huggingface/hub/datasets--HuggingFaceFW--fineweb-edu/blobs only has 1652.13 MB free disk space.\n",
      "  warnings.warn(\n",
      "/Users/michelangelozampieri/Desktop/gpt-course/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:799: UserWarning: Not enough free disk space to download the file. The expected file size is: 2152.32 MB. The target location /Users/michelangelozampieri/.cache/huggingface/hub/datasets--HuggingFaceFW--fineweb-edu/blobs only has 1653.03 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[129]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Login using e.g. `huggingface-cli login` to access this dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m ds = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHuggingFaceFW/fineweb-edu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msample-10BT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/datasets/load.py:2084\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2081\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance.as_streaming_dataset(split=split)\n\u001b[32m   2083\u001b[39m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2084\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2090\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2092\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   2093\u001b[39m keep_in_memory = (\n\u001b[32m   2094\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   2095\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/datasets/builder.py:925\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    924\u001b[39m     prepare_split_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_proc\u001b[39m\u001b[33m\"\u001b[39m] = num_proc\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[32m    932\u001b[39m \u001b[38;5;28mself\u001b[39m.info.dataset_size = \u001b[38;5;28msum\u001b[39m(split.num_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info.splits.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/datasets/builder.py:979\u001b[39m, in \u001b[36mDatasetBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[39m\n\u001b[32m    977\u001b[39m split_dict = SplitDict(dataset_name=\u001b[38;5;28mself\u001b[39m.dataset_name)\n\u001b[32m    978\u001b[39m split_generators_kwargs = \u001b[38;5;28mself\u001b[39m._make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m split_generators = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msplit_generators_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verification_mode == VerificationMode.ALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager.record_checksums:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/datasets/packaged_modules/parquet/parquet.py:49\u001b[39m, in \u001b[36mParquet._split_generators\u001b[39m\u001b[34m(self, dl_manager)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAt least one data file must be specified, but got data_files=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.data_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m dl_manager.download_config.extract_on_the_fly = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m data_files = \u001b[43mdl_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m splits = []\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split_name, files \u001b[38;5;129;01min\u001b[39;00m data_files.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/datasets/download/download_manager.py:326\u001b[39m, in \u001b[36mDownloadManager.download_and_extract\u001b[39m\u001b[34m(self, url_or_urls)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload_and_extract\u001b[39m(\u001b[38;5;28mself\u001b[39m, url_or_urls):\n\u001b[32m    311\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Download and extract given `url_or_urls`.\u001b[39;00m\n\u001b[32m    312\u001b[39m \n\u001b[32m    313\u001b[39m \u001b[33;03m    Is roughly equivalent to:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    324\u001b[39m \u001b[33;03m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.extract(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/datasets/download/download_manager.py:159\u001b[39m, in \u001b[36mDownloadManager.download\u001b[39m\u001b[34m(self, url_or_urls)\u001b[39m\n\u001b[32m    157\u001b[39m start_time = datetime.now()\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m stack_multiprocessing_download_progress_bars():\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     downloaded_path_or_paths = \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDownloading data files\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m duration = datetime.now() - start_time\n\u001b[32m    169\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration.total_seconds()\u001b[38;5;250m \u001b[39m//\u001b[38;5;250m \u001b[39m\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m min\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:505\u001b[39m, in \u001b[36mmap_nested\u001b[39m\u001b[34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[39m\n\u001b[32m    502\u001b[39m     num_proc = \u001b[32m1\u001b[39m\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(v, types) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(v) > \u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m iterable):\n\u001b[32m    504\u001b[39m     mapped = [\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m         \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparallel_min_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparallel_min_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    514\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m    515\u001b[39m     ]\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m num_proc != -\u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_proc <= \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(iterable) < parallel_min_length:\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m batched:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:522\u001b[39m, in \u001b[36mmap_nested\u001b[39m\u001b[34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[39m\n\u001b[32m    519\u001b[39m         batch_size = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) // num_proc + \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) % num_proc > \u001b[32m0\u001b[39m), \u001b[32m1\u001b[39m)\n\u001b[32m    520\u001b[39m     iterable = \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[32m    521\u001b[39m mapped = [\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m     \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable=disable_tqdm, desc=desc)\n\u001b[32m    524\u001b[39m ]\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    526\u001b[39m     mapped = [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:390\u001b[39m, in \u001b[36m_single_map_nested\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    383\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    385\u001b[39m     batched\n\u001b[32m    386\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    387\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[32m    388\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[32m    389\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m    392\u001b[39m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging.get_verbosity() < logging.WARNING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/datasets/download/download_manager.py:220\u001b[39m, in \u001b[36mDownloadManager._download_batched\u001b[39m\u001b[34m(self, url_or_filenames, download_config)\u001b[39m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[32m    207\u001b[39m         download_func,\n\u001b[32m    208\u001b[39m         url_or_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    216\u001b[39m         tqdm_class=tqdm,\n\u001b[32m    217\u001b[39m     )\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[32m    222\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/datasets/download/download_manager.py:229\u001b[39m, in \u001b[36mDownloadManager._download_single\u001b[39m\u001b[34m(self, url_or_filename, download_config)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# append the relative path to the base_path\u001b[39;00m\n\u001b[32m    228\u001b[39m     url_or_filename = url_or_path_join(\u001b[38;5;28mself\u001b[39m._base_path, url_or_filename)\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m out = \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m out = tracked_str(out)\n\u001b[32m    231\u001b[39m out.set_origin(url_or_filename)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/datasets/utils/file_utils.py:197\u001b[39m, in \u001b[36mcached_path\u001b[39m\u001b[34m(url_or_filename, download_config, **download_kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36m_inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:5486\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(self, repo_id, filename, subfolder, repo_type, revision, cache_dir, local_dir, force_download, proxies, etag_timeout, token, local_files_only, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36m_inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1008\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    989\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    990\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1005\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1006\u001b[39m     )\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1161\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1158\u001b[39m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[32m   1160\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1174\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1725\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1718\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1719\u001b[39m             logger.warning(\n\u001b[32m   1720\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo, but the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhf_xet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m package is not installed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1721\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFalling back to regular HTTP download. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1722\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1723\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1725\u001b[39m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1726\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1727\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1728\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1729\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1730\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1731\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1732\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1734\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1735\u001b[39m _chmod_and_move(incomplete_path, destination_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:497\u001b[39m, in \u001b[36mhttp_get\u001b[39m\u001b[34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[39m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[32m    496\u001b[39m     progress.update(\u001b[38;5;28mlen\u001b[39m(chunk))\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m     \u001b[43mtemp_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m     new_resume_size += \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[32m    499\u001b[39m     \u001b[38;5;66;03m# Some data has been downloaded from the server so we reset the number of retries.\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a92cea",
   "metadata": {},
   "source": [
    "Create the encoders and decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8b76fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { c: i for i, c in enumerate(chars) }\n",
    "int_to_string = { i: c for i, c in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad57416",
   "metadata": {},
   "source": [
    "Function to get a random chunk of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9b01221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_chunk(split):\n",
    "    filename = 'train_data.txt' if split == 'train' else 'val_data.txt'\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size * batch_size)\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', ' ')\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f957a971",
   "metadata": {},
   "source": [
    "Code to get a batch from the random chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9033138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1c8507f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_token(logits, temperature=1.0, top_p=0.9):\n",
    "    logits = logits / temperature\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    logits[..., indices_to_remove] = -float('Inf')\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574acf0",
   "metadata": {},
   "source": [
    "Define the classes for the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "38f89abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "184d8da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f2d866f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d70a8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c011ec98",
   "metadata": {},
   "source": [
    "Here define the model and load it from the pretrained params and send it to the device to allow training on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "        \n",
    "    def generate(self, index, max_new_tokens, temperature=1.0, top_p=0.9):\n",
    "        for _ in range(max_new_tokens):\n",
    "            index_cond = index[:, -block_size:]\n",
    "            logits, _ = self.forward(index_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            next_token = sample_next_token(logits, temperature=temperature, top_p=top_p)\n",
    "            index = torch.cat((index, next_token), dim=1)\n",
    "        return index\n",
    "\n",
    "\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "\n",
    "# with open('model-02.pkl', 'rb') as f:\n",
    "#     model = pickle.load(f)\n",
    "\n",
    "# print('Model loaded successfully.')\n",
    "\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a01c779",
   "metadata": {},
   "source": [
    "Function to estimate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "02bfd370",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            _, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        mean_loss = losses.mean()\n",
    "        perplexity = torch.exp(mean_loss)\n",
    "        out[split] = (mean_loss.item(), perplexity.item())\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c380c",
   "metadata": {},
   "source": [
    "Define the optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5cfffffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e760acb",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d9440a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0] Train Loss: 4.916, Val Loss: 4.916 | Train PPL: 136.51, Val PPL: 136.46\n",
      "[Step 100] Train Loss: 2.816, Val Loss: 2.818 | Train PPL: 16.71, Val PPL: 16.74\n",
      "[Step 200] Train Loss: 2.637, Val Loss: 2.631 | Train PPL: 13.98, Val PPL: 13.88\n",
      "[Step 300] Train Loss: 2.513, Val Loss: 2.512 | Train PPL: 12.35, Val PPL: 12.33\n",
      "[Step 400] Train Loss: 2.423, Val Loss: 2.438 | Train PPL: 11.28, Val PPL: 11.45\n",
      "[Step 500] Train Loss: 2.394, Val Loss: 2.396 | Train PPL: 10.95, Val PPL: 10.98\n",
      "[Step 600] Train Loss: 2.367, Val Loss: 2.369 | Train PPL: 10.66, Val PPL: 10.69\n",
      "[Step 700] Train Loss: 2.315, Val Loss: 2.331 | Train PPL: 10.12, Val PPL: 10.29\n",
      "[Step 800] Train Loss: 2.284, Val Loss: 2.285 | Train PPL: 9.81, Val PPL: 9.83\n",
      "[Step 900] Train Loss: 2.269, Val Loss: 2.261 | Train PPL: 9.67, Val PPL: 9.59\n",
      "[Step 1000] Train Loss: 2.227, Val Loss: 2.231 | Train PPL: 9.28, Val PPL: 9.31\n",
      "[Step 1100] Train Loss: 2.203, Val Loss: 2.194 | Train PPL: 9.05, Val PPL: 8.97\n",
      "[Step 1200] Train Loss: 2.157, Val Loss: 2.177 | Train PPL: 8.64, Val PPL: 8.82\n",
      "[Step 1300] Train Loss: 2.153, Val Loss: 2.149 | Train PPL: 8.61, Val PPL: 8.58\n",
      "[Step 1400] Train Loss: 2.132, Val Loss: 2.131 | Train PPL: 8.43, Val PPL: 8.42\n",
      "[Step 1500] Train Loss: 2.130, Val Loss: 2.114 | Train PPL: 8.41, Val PPL: 8.28\n",
      "[Step 1600] Train Loss: 2.101, Val Loss: 2.100 | Train PPL: 8.17, Val PPL: 8.16\n",
      "[Step 1700] Train Loss: 2.105, Val Loss: 2.090 | Train PPL: 8.21, Val PPL: 8.09\n",
      "[Step 1800] Train Loss: 2.089, Val Loss: 2.079 | Train PPL: 8.07, Val PPL: 8.00\n",
      "[Step 1900] Train Loss: 2.077, Val Loss: 2.060 | Train PPL: 7.98, Val PPL: 7.84\n",
      "[Step 2000] Train Loss: 2.057, Val Loss: 2.064 | Train PPL: 7.82, Val PPL: 7.88\n",
      "[Step 2100] Train Loss: 2.041, Val Loss: 2.031 | Train PPL: 7.70, Val PPL: 7.62\n",
      "[Step 2200] Train Loss: 2.026, Val Loss: 2.036 | Train PPL: 7.58, Val PPL: 7.66\n",
      "[Step 2300] Train Loss: 2.019, Val Loss: 2.031 | Train PPL: 7.53, Val PPL: 7.62\n",
      "[Step 2400] Train Loss: 2.015, Val Loss: 2.014 | Train PPL: 7.50, Val PPL: 7.49\n",
      "[Step 2500] Train Loss: 2.010, Val Loss: 2.008 | Train PPL: 7.47, Val PPL: 7.45\n",
      "[Step 2600] Train Loss: 1.999, Val Loss: 2.006 | Train PPL: 7.38, Val PPL: 7.43\n",
      "[Step 2700] Train Loss: 1.991, Val Loss: 1.990 | Train PPL: 7.32, Val PPL: 7.32\n",
      "[Step 2800] Train Loss: 1.990, Val Loss: 1.997 | Train PPL: 7.31, Val PPL: 7.36\n",
      "[Step 2900] Train Loss: 1.996, Val Loss: 1.973 | Train PPL: 7.36, Val PPL: 7.19\n",
      "[Step 3000] Train Loss: 1.985, Val Loss: 1.983 | Train PPL: 7.28, Val PPL: 7.27\n",
      "[Step 3100] Train Loss: 1.967, Val Loss: 1.974 | Train PPL: 7.15, Val PPL: 7.20\n",
      "[Step 3200] Train Loss: 1.980, Val Loss: 1.967 | Train PPL: 7.24, Val PPL: 7.15\n",
      "[Step 3300] Train Loss: 1.963, Val Loss: 1.956 | Train PPL: 7.12, Val PPL: 7.07\n",
      "[Step 3400] Train Loss: 1.947, Val Loss: 1.954 | Train PPL: 7.01, Val PPL: 7.06\n",
      "[Step 3500] Train Loss: 1.953, Val Loss: 1.940 | Train PPL: 7.05, Val PPL: 6.96\n",
      "[Step 3600] Train Loss: 1.965, Val Loss: 1.959 | Train PPL: 7.14, Val PPL: 7.09\n",
      "[Step 3700] Train Loss: 1.932, Val Loss: 1.944 | Train PPL: 6.91, Val PPL: 6.98\n",
      "[Step 3800] Train Loss: 1.933, Val Loss: 1.942 | Train PPL: 6.91, Val PPL: 6.97\n",
      "[Step 3900] Train Loss: 1.929, Val Loss: 1.930 | Train PPL: 6.88, Val PPL: 6.89\n",
      "[Step 4000] Train Loss: 1.929, Val Loss: 1.912 | Train PPL: 6.88, Val PPL: 6.77\n",
      "[Step 4100] Train Loss: 1.919, Val Loss: 1.926 | Train PPL: 6.81, Val PPL: 6.86\n",
      "[Step 4200] Train Loss: 1.929, Val Loss: 1.913 | Train PPL: 6.88, Val PPL: 6.77\n",
      "[Step 4300] Train Loss: 1.924, Val Loss: 1.913 | Train PPL: 6.85, Val PPL: 6.77\n",
      "[Step 4400] Train Loss: 1.917, Val Loss: 1.913 | Train PPL: 6.80, Val PPL: 6.77\n",
      "[Step 4500] Train Loss: 1.897, Val Loss: 1.908 | Train PPL: 6.67, Val PPL: 6.74\n",
      "[Step 4600] Train Loss: 1.909, Val Loss: 1.896 | Train PPL: 6.74, Val PPL: 6.66\n",
      "[Step 4700] Train Loss: 1.913, Val Loss: 1.899 | Train PPL: 6.77, Val PPL: 6.68\n",
      "[Step 4800] Train Loss: 1.899, Val Loss: 1.898 | Train PPL: 6.68, Val PPL: 6.67\n",
      "[Step 4900] Train Loss: 1.893, Val Loss: 1.900 | Train PPL: 6.64, Val PPL: 6.69\n",
      "[Step 5000] Train Loss: 1.898, Val Loss: 1.900 | Train PPL: 6.67, Val PPL: 6.68\n",
      "[Step 5100] Train Loss: 1.887, Val Loss: 1.894 | Train PPL: 6.60, Val PPL: 6.65\n",
      "[Step 5200] Train Loss: 1.894, Val Loss: 1.890 | Train PPL: 6.65, Val PPL: 6.62\n",
      "[Step 5300] Train Loss: 1.886, Val Loss: 1.881 | Train PPL: 6.59, Val PPL: 6.56\n",
      "[Step 5400] Train Loss: 1.868, Val Loss: 1.876 | Train PPL: 6.47, Val PPL: 6.53\n",
      "[Step 5500] Train Loss: 1.867, Val Loss: 1.878 | Train PPL: 6.47, Val PPL: 6.54\n",
      "[Step 5600] Train Loss: 1.869, Val Loss: 1.868 | Train PPL: 6.48, Val PPL: 6.48\n",
      "[Step 5700] Train Loss: 1.879, Val Loss: 1.877 | Train PPL: 6.55, Val PPL: 6.53\n",
      "[Step 5800] Train Loss: 1.874, Val Loss: 1.869 | Train PPL: 6.51, Val PPL: 6.48\n",
      "[Step 5900] Train Loss: 1.859, Val Loss: 1.876 | Train PPL: 6.42, Val PPL: 6.53\n",
      "[Step 6000] Train Loss: 1.859, Val Loss: 1.857 | Train PPL: 6.41, Val PPL: 6.41\n",
      "[Step 6100] Train Loss: 1.848, Val Loss: 1.856 | Train PPL: 6.35, Val PPL: 6.40\n",
      "[Step 6200] Train Loss: 1.860, Val Loss: 1.857 | Train PPL: 6.43, Val PPL: 6.41\n",
      "[Step 6300] Train Loss: 1.848, Val Loss: 1.853 | Train PPL: 6.35, Val PPL: 6.38\n",
      "[Step 6400] Train Loss: 1.867, Val Loss: 1.848 | Train PPL: 6.47, Val PPL: 6.35\n",
      "[Step 6500] Train Loss: 1.855, Val Loss: 1.864 | Train PPL: 6.39, Val PPL: 6.45\n",
      "[Step 6600] Train Loss: 1.861, Val Loss: 1.860 | Train PPL: 6.43, Val PPL: 6.43\n",
      "[Step 6700] Train Loss: 1.846, Val Loss: 1.852 | Train PPL: 6.34, Val PPL: 6.37\n",
      "[Step 6800] Train Loss: 1.857, Val Loss: 1.847 | Train PPL: 6.40, Val PPL: 6.34\n",
      "[Step 6900] Train Loss: 1.845, Val Loss: 1.844 | Train PPL: 6.33, Val PPL: 6.32\n",
      "[Step 7000] Train Loss: 1.853, Val Loss: 1.854 | Train PPL: 6.38, Val PPL: 6.39\n",
      "[Step 7100] Train Loss: 1.852, Val Loss: 1.837 | Train PPL: 6.37, Val PPL: 6.28\n",
      "[Step 7200] Train Loss: 1.843, Val Loss: 1.829 | Train PPL: 6.32, Val PPL: 6.22\n",
      "[Step 7300] Train Loss: 1.843, Val Loss: 1.846 | Train PPL: 6.32, Val PPL: 6.34\n",
      "[Step 7400] Train Loss: 1.846, Val Loss: 1.834 | Train PPL: 6.34, Val PPL: 6.26\n",
      "[Step 7500] Train Loss: 1.839, Val Loss: 1.828 | Train PPL: 6.29, Val PPL: 6.22\n",
      "[Step 7600] Train Loss: 1.830, Val Loss: 1.825 | Train PPL: 6.24, Val PPL: 6.20\n",
      "[Step 7700] Train Loss: 1.829, Val Loss: 1.830 | Train PPL: 6.23, Val PPL: 6.23\n",
      "[Step 7800] Train Loss: 1.829, Val Loss: 1.830 | Train PPL: 6.23, Val PPL: 6.23\n",
      "[Step 7900] Train Loss: 1.820, Val Loss: 1.819 | Train PPL: 6.17, Val PPL: 6.17\n",
      "[Step 8000] Train Loss: 1.842, Val Loss: 1.826 | Train PPL: 6.31, Val PPL: 6.21\n",
      "[Step 8100] Train Loss: 1.816, Val Loss: 1.805 | Train PPL: 6.14, Val PPL: 6.08\n",
      "[Step 8200] Train Loss: 1.826, Val Loss: 1.821 | Train PPL: 6.21, Val PPL: 6.18\n",
      "[Step 8300] Train Loss: 1.805, Val Loss: 1.831 | Train PPL: 6.08, Val PPL: 6.24\n",
      "[Step 8400] Train Loss: 1.815, Val Loss: 1.812 | Train PPL: 6.14, Val PPL: 6.12\n",
      "[Step 8500] Train Loss: 1.805, Val Loss: 1.824 | Train PPL: 6.08, Val PPL: 6.20\n",
      "[Step 8600] Train Loss: 1.814, Val Loss: 1.807 | Train PPL: 6.13, Val PPL: 6.09\n",
      "[Step 8700] Train Loss: 1.812, Val Loss: 1.810 | Train PPL: 6.12, Val PPL: 6.11\n",
      "[Step 8800] Train Loss: 1.821, Val Loss: 1.811 | Train PPL: 6.18, Val PPL: 6.11\n",
      "[Step 8900] Train Loss: 1.817, Val Loss: 1.799 | Train PPL: 6.15, Val PPL: 6.05\n",
      "[Step 9000] Train Loss: 1.818, Val Loss: 1.805 | Train PPL: 6.16, Val PPL: 6.08\n",
      "[Step 9100] Train Loss: 1.796, Val Loss: 1.805 | Train PPL: 6.02, Val PPL: 6.08\n",
      "[Step 9200] Train Loss: 1.794, Val Loss: 1.799 | Train PPL: 6.01, Val PPL: 6.04\n",
      "[Step 9300] Train Loss: 1.797, Val Loss: 1.805 | Train PPL: 6.03, Val PPL: 6.08\n",
      "[Step 9400] Train Loss: 1.793, Val Loss: 1.806 | Train PPL: 6.01, Val PPL: 6.08\n",
      "[Step 9500] Train Loss: 1.805, Val Loss: 1.812 | Train PPL: 6.08, Val PPL: 6.12\n",
      "[Step 9600] Train Loss: 1.809, Val Loss: 1.799 | Train PPL: 6.10, Val PPL: 6.05\n",
      "[Step 9700] Train Loss: 1.790, Val Loss: 1.798 | Train PPL: 5.99, Val PPL: 6.04\n",
      "[Step 9800] Train Loss: 1.784, Val Loss: 1.793 | Train PPL: 5.95, Val PPL: 6.01\n",
      "[Step 9900] Train Loss: 1.794, Val Loss: 1.803 | Train PPL: 6.02, Val PPL: 6.07\n",
      "[Step 10000] Train Loss: 1.791, Val Loss: 1.799 | Train PPL: 5.99, Val PPL: 6.05\n",
      "[Step 10100] Train Loss: 1.802, Val Loss: 1.787 | Train PPL: 6.06, Val PPL: 5.97\n",
      "[Step 10200] Train Loss: 1.795, Val Loss: 1.806 | Train PPL: 6.02, Val PPL: 6.09\n",
      "[Step 10300] Train Loss: 1.778, Val Loss: 1.787 | Train PPL: 5.92, Val PPL: 5.97\n",
      "[Step 10400] Train Loss: 1.793, Val Loss: 1.781 | Train PPL: 6.01, Val PPL: 5.94\n",
      "[Step 10500] Train Loss: 1.791, Val Loss: 1.797 | Train PPL: 6.00, Val PPL: 6.03\n",
      "[Step 10600] Train Loss: 1.793, Val Loss: 1.791 | Train PPL: 6.01, Val PPL: 6.00\n",
      "[Step 10700] Train Loss: 1.785, Val Loss: 1.786 | Train PPL: 5.96, Val PPL: 5.96\n",
      "[Step 10800] Train Loss: 1.791, Val Loss: 1.789 | Train PPL: 6.00, Val PPL: 5.98\n",
      "[Step 10900] Train Loss: 1.778, Val Loss: 1.793 | Train PPL: 5.92, Val PPL: 6.01\n",
      "[Step 11000] Train Loss: 1.775, Val Loss: 1.780 | Train PPL: 5.90, Val PPL: 5.93\n",
      "[Step 11100] Train Loss: 1.777, Val Loss: 1.788 | Train PPL: 5.91, Val PPL: 5.98\n",
      "[Step 11200] Train Loss: 1.780, Val Loss: 1.788 | Train PPL: 5.93, Val PPL: 5.98\n",
      "[Step 11300] Train Loss: 1.772, Val Loss: 1.777 | Train PPL: 5.88, Val PPL: 5.91\n",
      "[Step 11400] Train Loss: 1.772, Val Loss: 1.773 | Train PPL: 5.88, Val PPL: 5.89\n",
      "[Step 11500] Train Loss: 1.769, Val Loss: 1.777 | Train PPL: 5.87, Val PPL: 5.91\n",
      "[Step 11600] Train Loss: 1.775, Val Loss: 1.774 | Train PPL: 5.90, Val PPL: 5.90\n",
      "[Step 11700] Train Loss: 1.782, Val Loss: 1.780 | Train PPL: 5.94, Val PPL: 5.93\n",
      "[Step 11800] Train Loss: 1.769, Val Loss: 1.783 | Train PPL: 5.86, Val PPL: 5.95\n",
      "[Step 11900] Train Loss: 1.777, Val Loss: 1.783 | Train PPL: 5.91, Val PPL: 5.95\n",
      "[Step 12000] Train Loss: 1.780, Val Loss: 1.783 | Train PPL: 5.93, Val PPL: 5.95\n",
      "[Step 12100] Train Loss: 1.770, Val Loss: 1.773 | Train PPL: 5.87, Val PPL: 5.89\n",
      "[Step 12200] Train Loss: 1.789, Val Loss: 1.768 | Train PPL: 5.99, Val PPL: 5.86\n",
      "[Step 12300] Train Loss: 1.776, Val Loss: 1.766 | Train PPL: 5.90, Val PPL: 5.85\n",
      "[Step 12400] Train Loss: 1.764, Val Loss: 1.784 | Train PPL: 5.84, Val PPL: 5.96\n",
      "[Step 12500] Train Loss: 1.773, Val Loss: 1.788 | Train PPL: 5.89, Val PPL: 5.98\n",
      "[Step 12600] Train Loss: 1.772, Val Loss: 1.760 | Train PPL: 5.88, Val PPL: 5.81\n",
      "[Step 12700] Train Loss: 1.776, Val Loss: 1.757 | Train PPL: 5.91, Val PPL: 5.80\n",
      "[Step 12800] Train Loss: 1.758, Val Loss: 1.768 | Train PPL: 5.80, Val PPL: 5.86\n",
      "[Step 12900] Train Loss: 1.766, Val Loss: 1.757 | Train PPL: 5.85, Val PPL: 5.79\n",
      "[Step 13000] Train Loss: 1.774, Val Loss: 1.757 | Train PPL: 5.89, Val PPL: 5.80\n",
      "[Step 13100] Train Loss: 1.760, Val Loss: 1.761 | Train PPL: 5.81, Val PPL: 5.82\n",
      "[Step 13200] Train Loss: 1.755, Val Loss: 1.763 | Train PPL: 5.78, Val PPL: 5.83\n",
      "[Step 13300] Train Loss: 1.756, Val Loss: 1.776 | Train PPL: 5.79, Val PPL: 5.91\n",
      "[Step 13400] Train Loss: 1.771, Val Loss: 1.758 | Train PPL: 5.88, Val PPL: 5.80\n",
      "[Step 13500] Train Loss: 1.768, Val Loss: 1.761 | Train PPL: 5.86, Val PPL: 5.82\n",
      "[Step 13600] Train Loss: 1.756, Val Loss: 1.746 | Train PPL: 5.79, Val PPL: 5.73\n",
      "[Step 13700] Train Loss: 1.761, Val Loss: 1.741 | Train PPL: 5.82, Val PPL: 5.70\n",
      "[Step 13800] Train Loss: 1.776, Val Loss: 1.770 | Train PPL: 5.90, Val PPL: 5.87\n",
      "[Step 13900] Train Loss: 1.742, Val Loss: 1.762 | Train PPL: 5.71, Val PPL: 5.82\n",
      "[Step 14000] Train Loss: 1.756, Val Loss: 1.757 | Train PPL: 5.79, Val PPL: 5.79\n",
      "[Step 14100] Train Loss: 1.741, Val Loss: 1.756 | Train PPL: 5.70, Val PPL: 5.79\n",
      "[Step 14200] Train Loss: 1.761, Val Loss: 1.764 | Train PPL: 5.82, Val PPL: 5.84\n",
      "[Step 14300] Train Loss: 1.745, Val Loss: 1.753 | Train PPL: 5.73, Val PPL: 5.77\n",
      "[Step 14400] Train Loss: 1.765, Val Loss: 1.748 | Train PPL: 5.84, Val PPL: 5.74\n",
      "[Step 14500] Train Loss: 1.751, Val Loss: 1.752 | Train PPL: 5.76, Val PPL: 5.77\n",
      "[Step 14600] Train Loss: 1.756, Val Loss: 1.762 | Train PPL: 5.79, Val PPL: 5.82\n",
      "[Step 14700] Train Loss: 1.754, Val Loss: 1.737 | Train PPL: 5.78, Val PPL: 5.68\n",
      "[Step 14800] Train Loss: 1.756, Val Loss: 1.742 | Train PPL: 5.79, Val PPL: 5.71\n",
      "[Step 14900] Train Loss: 1.750, Val Loss: 1.765 | Train PPL: 5.76, Val PPL: 5.84\n",
      "[Step 15000] Train Loss: 1.752, Val Loss: 1.739 | Train PPL: 5.77, Val PPL: 5.69\n",
      "[Step 15100] Train Loss: 1.747, Val Loss: 1.764 | Train PPL: 5.74, Val PPL: 5.83\n",
      "[Step 15200] Train Loss: 1.742, Val Loss: 1.754 | Train PPL: 5.71, Val PPL: 5.78\n",
      "[Step 15300] Train Loss: 1.758, Val Loss: 1.755 | Train PPL: 5.80, Val PPL: 5.78\n",
      "[Step 15400] Train Loss: 1.753, Val Loss: 1.752 | Train PPL: 5.77, Val PPL: 5.77\n",
      "[Step 15500] Train Loss: 1.759, Val Loss: 1.736 | Train PPL: 5.81, Val PPL: 5.68\n",
      "[Step 15600] Train Loss: 1.754, Val Loss: 1.746 | Train PPL: 5.78, Val PPL: 5.73\n",
      "[Step 15700] Train Loss: 1.738, Val Loss: 1.744 | Train PPL: 5.69, Val PPL: 5.72\n",
      "[Step 15800] Train Loss: 1.734, Val Loss: 1.739 | Train PPL: 5.67, Val PPL: 5.69\n",
      "[Step 15900] Train Loss: 1.736, Val Loss: 1.736 | Train PPL: 5.68, Val PPL: 5.67\n",
      "[Step 16000] Train Loss: 1.744, Val Loss: 1.745 | Train PPL: 5.72, Val PPL: 5.73\n",
      "[Step 16100] Train Loss: 1.731, Val Loss: 1.753 | Train PPL: 5.65, Val PPL: 5.77\n",
      "[Step 16200] Train Loss: 1.745, Val Loss: 1.757 | Train PPL: 5.73, Val PPL: 5.79\n",
      "[Step 16300] Train Loss: 1.752, Val Loss: 1.743 | Train PPL: 5.76, Val PPL: 5.71\n",
      "[Step 16400] Train Loss: 1.738, Val Loss: 1.747 | Train PPL: 5.69, Val PPL: 5.74\n",
      "[Step 16500] Train Loss: 1.741, Val Loss: 1.741 | Train PPL: 5.70, Val PPL: 5.70\n",
      "[Step 16600] Train Loss: 1.753, Val Loss: 1.743 | Train PPL: 5.77, Val PPL: 5.72\n",
      "[Step 16700] Train Loss: 1.745, Val Loss: 1.725 | Train PPL: 5.73, Val PPL: 5.62\n",
      "[Step 16800] Train Loss: 1.749, Val Loss: 1.743 | Train PPL: 5.75, Val PPL: 5.71\n",
      "[Step 16900] Train Loss: 1.738, Val Loss: 1.741 | Train PPL: 5.69, Val PPL: 5.71\n",
      "[Step 17000] Train Loss: 1.737, Val Loss: 1.748 | Train PPL: 5.68, Val PPL: 5.74\n",
      "[Step 17100] Train Loss: 1.751, Val Loss: 1.732 | Train PPL: 5.76, Val PPL: 5.65\n",
      "[Step 17200] Train Loss: 1.743, Val Loss: 1.732 | Train PPL: 5.71, Val PPL: 5.65\n",
      "[Step 17300] Train Loss: 1.742, Val Loss: 1.739 | Train PPL: 5.71, Val PPL: 5.69\n",
      "[Step 17400] Train Loss: 1.747, Val Loss: 1.746 | Train PPL: 5.74, Val PPL: 5.73\n",
      "[Step 17500] Train Loss: 1.727, Val Loss: 1.736 | Train PPL: 5.62, Val PPL: 5.67\n",
      "[Step 17600] Train Loss: 1.740, Val Loss: 1.746 | Train PPL: 5.69, Val PPL: 5.73\n",
      "[Step 17700] Train Loss: 1.718, Val Loss: 1.736 | Train PPL: 5.57, Val PPL: 5.68\n",
      "[Step 17800] Train Loss: 1.722, Val Loss: 1.735 | Train PPL: 5.59, Val PPL: 5.67\n",
      "[Step 17900] Train Loss: 1.736, Val Loss: 1.735 | Train PPL: 5.68, Val PPL: 5.67\n",
      "[Step 18000] Train Loss: 1.734, Val Loss: 1.737 | Train PPL: 5.66, Val PPL: 5.68\n",
      "[Step 18100] Train Loss: 1.728, Val Loss: 1.732 | Train PPL: 5.63, Val PPL: 5.65\n",
      "[Step 18200] Train Loss: 1.734, Val Loss: 1.726 | Train PPL: 5.66, Val PPL: 5.62\n",
      "[Step 18300] Train Loss: 1.739, Val Loss: 1.729 | Train PPL: 5.69, Val PPL: 5.64\n",
      "[Step 18400] Train Loss: 1.735, Val Loss: 1.730 | Train PPL: 5.67, Val PPL: 5.64\n",
      "[Step 18500] Train Loss: 1.738, Val Loss: 1.722 | Train PPL: 5.68, Val PPL: 5.59\n",
      "[Step 18600] Train Loss: 1.733, Val Loss: 1.727 | Train PPL: 5.66, Val PPL: 5.62\n",
      "[Step 18700] Train Loss: 1.735, Val Loss: 1.726 | Train PPL: 5.67, Val PPL: 5.62\n",
      "[Step 18800] Train Loss: 1.741, Val Loss: 1.731 | Train PPL: 5.71, Val PPL: 5.65\n",
      "[Step 18900] Train Loss: 1.732, Val Loss: 1.730 | Train PPL: 5.65, Val PPL: 5.64\n",
      "[Step 19000] Train Loss: 1.728, Val Loss: 1.745 | Train PPL: 5.63, Val PPL: 5.73\n",
      "[Step 19100] Train Loss: 1.732, Val Loss: 1.728 | Train PPL: 5.65, Val PPL: 5.63\n",
      "[Step 19200] Train Loss: 1.719, Val Loss: 1.725 | Train PPL: 5.58, Val PPL: 5.61\n",
      "[Step 19300] Train Loss: 1.726, Val Loss: 1.708 | Train PPL: 5.62, Val PPL: 5.52\n",
      "[Step 19400] Train Loss: 1.731, Val Loss: 1.730 | Train PPL: 5.64, Val PPL: 5.64\n",
      "[Step 19500] Train Loss: 1.728, Val Loss: 1.731 | Train PPL: 5.63, Val PPL: 5.65\n",
      "[Step 19600] Train Loss: 1.732, Val Loss: 1.720 | Train PPL: 5.65, Val PPL: 5.58\n",
      "[Step 19700] Train Loss: 1.734, Val Loss: 1.724 | Train PPL: 5.67, Val PPL: 5.60\n",
      "[Step 19800] Train Loss: 1.740, Val Loss: 1.729 | Train PPL: 5.70, Val PPL: 5.64\n",
      "[Step 19900] Train Loss: 1.738, Val Loss: 1.710 | Train PPL: 5.69, Val PPL: 5.53\n",
      "[Step 20000] Train Loss: 1.722, Val Loss: 1.740 | Train PPL: 5.60, Val PPL: 5.70\n",
      "[Step 20100] Train Loss: 1.730, Val Loss: 1.719 | Train PPL: 5.64, Val PPL: 5.58\n",
      "[Step 20200] Train Loss: 1.717, Val Loss: 1.739 | Train PPL: 5.57, Val PPL: 5.69\n",
      "[Step 20300] Train Loss: 1.723, Val Loss: 1.724 | Train PPL: 5.60, Val PPL: 5.61\n",
      "[Step 20400] Train Loss: 1.733, Val Loss: 1.715 | Train PPL: 5.66, Val PPL: 5.56\n",
      "[Step 20500] Train Loss: 1.723, Val Loss: 1.726 | Train PPL: 5.60, Val PPL: 5.62\n",
      "[Step 20600] Train Loss: 1.724, Val Loss: 1.726 | Train PPL: 5.61, Val PPL: 5.62\n",
      "[Step 20700] Train Loss: 1.720, Val Loss: 1.717 | Train PPL: 5.58, Val PPL: 5.57\n",
      "[Step 20800] Train Loss: 1.719, Val Loss: 1.717 | Train PPL: 5.58, Val PPL: 5.57\n",
      "[Step 20900] Train Loss: 1.720, Val Loss: 1.727 | Train PPL: 5.59, Val PPL: 5.63\n",
      "[Step 21000] Train Loss: 1.719, Val Loss: 1.712 | Train PPL: 5.58, Val PPL: 5.54\n",
      "[Step 21100] Train Loss: 1.724, Val Loss: 1.718 | Train PPL: 5.61, Val PPL: 5.57\n",
      "[Step 21200] Train Loss: 1.728, Val Loss: 1.730 | Train PPL: 5.63, Val PPL: 5.64\n",
      "[Step 21300] Train Loss: 1.719, Val Loss: 1.720 | Train PPL: 5.58, Val PPL: 5.58\n",
      "[Step 21400] Train Loss: 1.735, Val Loss: 1.719 | Train PPL: 5.67, Val PPL: 5.58\n",
      "[Step 21500] Train Loss: 1.730, Val Loss: 1.724 | Train PPL: 5.64, Val PPL: 5.61\n",
      "[Step 21600] Train Loss: 1.726, Val Loss: 1.711 | Train PPL: 5.62, Val PPL: 5.53\n",
      "[Step 21700] Train Loss: 1.736, Val Loss: 1.727 | Train PPL: 5.67, Val PPL: 5.62\n",
      "[Step 21800] Train Loss: 1.727, Val Loss: 1.718 | Train PPL: 5.62, Val PPL: 5.57\n",
      "[Step 21900] Train Loss: 1.724, Val Loss: 1.726 | Train PPL: 5.61, Val PPL: 5.62\n",
      "[Step 22000] Train Loss: 1.722, Val Loss: 1.719 | Train PPL: 5.59, Val PPL: 5.58\n",
      "[Step 22100] Train Loss: 1.720, Val Loss: 1.722 | Train PPL: 5.58, Val PPL: 5.60\n",
      "[Step 22200] Train Loss: 1.717, Val Loss: 1.725 | Train PPL: 5.57, Val PPL: 5.61\n",
      "[Step 22300] Train Loss: 1.730, Val Loss: 1.724 | Train PPL: 5.64, Val PPL: 5.61\n",
      "[Step 22400] Train Loss: 1.729, Val Loss: 1.728 | Train PPL: 5.64, Val PPL: 5.63\n",
      "[Step 22500] Train Loss: 1.721, Val Loss: 1.723 | Train PPL: 5.59, Val PPL: 5.60\n",
      "[Step 22600] Train Loss: 1.715, Val Loss: 1.730 | Train PPL: 5.56, Val PPL: 5.64\n",
      "[Step 22700] Train Loss: 1.711, Val Loss: 1.719 | Train PPL: 5.54, Val PPL: 5.58\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m logits, loss = m(xb, yb)\n\u001b[32m     10\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m optimizer.step()\n\u001b[32m     13\u001b[39m scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gpt-course/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        train_loss, train_ppl = losses['train']\n",
    "        val_loss, val_ppl = losses['val']\n",
    "        print(f\"[Step {iter}] Train Loss: {train_loss:.3f}, Val Loss: {val_loss:.3f} | Train PPL: {train_ppl:.2f}, Val PPL: {val_ppl:.2f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c395cb",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f48c5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model-03.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65560a",
   "metadata": {},
   "source": [
    "Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "25cc39e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t. Looking gentle, â€œthat was the dremble.\n",
      "Englation was the land subjected about\n",
      "cours a mad she felt to be to whole fire. In them she lead not damply and the brough of itstophing and passas chippear we had\n",
      "they one above the rightled of hone agate seemeding infell and everage will pan a losse\n",
      "wanter down ration her have a spipent it a candow.â€\n",
      "I that is when the countaine. The prichman on the blookh ancous him this collding to his browere are where it chour, to save\n",
      "HEMES.\n",
      "resendentional docting\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82526f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
