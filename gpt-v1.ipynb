{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86a8f26",
   "metadata": {},
   "source": [
    "# Pre Training Custom GPT LLM\n",
    "\n",
    "## Author: Michelangelo Zampieri\n",
    "\n",
    "This notebook contains code to build a custom gpt LLM. \n",
    "\n",
    "The code was generated following the youtube tutorial \"Create a Large Language Model from Scratch with Python – Tutorial\" by freeCodeCamp.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23b07d",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eea6cff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7135988",
   "metadata": {},
   "source": [
    "Define hyper paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c09364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 32\n",
    "batch_size = 32\n",
    "max_iters = 50000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 100\n",
    "n_embd = 128\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc81c7c",
   "metadata": {},
   "source": [
    "Read the vocab text and create a sorted array of chars and get its size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c6d6d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 12788\n"
     ]
    }
   ],
   "source": [
    "chars = set()\n",
    "with open('crawl_data_train.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        chars.update(line)\n",
    "chars = sorted(list(chars))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a92cea",
   "metadata": {},
   "source": [
    "Create the encoders and decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b76fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { c: i for i, c in enumerate(chars) }\n",
    "int_to_string = { i: c for i, c in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad57416",
   "metadata": {},
   "source": [
    "Function to get a random chunk of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b01221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_chunk(split):\n",
    "    filename = 'crawl_data_train.txt'\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size * batch_size)\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', ' ')\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f957a971",
   "metadata": {},
   "source": [
    "Code to get a batch from the random chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9033138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c8507f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_token(logits, temperature=1.0, top_p=0.9):\n",
    "    logits = logits / temperature\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    logits[..., indices_to_remove] = -float('Inf')\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574acf0",
   "metadata": {},
   "source": [
    "Define the classes for the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38f89abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "184d8da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2d866f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d70a8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c011ec98",
   "metadata": {},
   "source": [
    "Here define the model and load it from the pretrained params and send it to the device to allow training on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9539c55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.08242 M parameters\n"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "        \n",
    "    def generate(self, index, max_new_tokens, temperature=1.0, top_p=0.9):\n",
    "        for _ in range(max_new_tokens):\n",
    "            index_cond = index[:, -block_size:]\n",
    "            logits, _ = self.forward(index_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            next_token = sample_next_token(logits, temperature=temperature, top_p=top_p)\n",
    "            index = torch.cat((index, next_token), dim=1)\n",
    "        return index\n",
    "\n",
    "\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "\n",
    "# with open('model-02.pkl', 'rb') as f:\n",
    "#     model = pickle.load(f)\n",
    "\n",
    "# print('Model loaded successfully.')\n",
    "\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a01c779",
   "metadata": {},
   "source": [
    "Function to estimate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02bfd370",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            _, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        mean_loss = losses.mean()\n",
    "        perplexity = torch.exp(mean_loss)\n",
    "        out[split] = (mean_loss.item(), perplexity.item())\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c380c",
   "metadata": {},
   "source": [
    "Define the optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfffffd",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e760acb",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9440a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0] Train Loss: 9.461, Val Loss: 9.461 | Train PPL: 12853.97, Val PPL: 12849.79\n",
      "[Step 100] Train Loss: 3.306, Val Loss: 3.330 | Train PPL: 27.26, Val PPL: 27.94\n",
      "[Step 200] Train Loss: 2.909, Val Loss: 2.910 | Train PPL: 18.33, Val PPL: 18.35\n",
      "[Step 300] Train Loss: 2.739, Val Loss: 2.735 | Train PPL: 15.47, Val PPL: 15.41\n",
      "[Step 400] Train Loss: 2.651, Val Loss: 2.651 | Train PPL: 14.17, Val PPL: 14.16\n",
      "[Step 500] Train Loss: 2.582, Val Loss: 2.577 | Train PPL: 13.22, Val PPL: 13.16\n",
      "[Step 600] Train Loss: 2.489, Val Loss: 2.517 | Train PPL: 12.05, Val PPL: 12.39\n",
      "[Step 700] Train Loss: 2.470, Val Loss: 2.457 | Train PPL: 11.82, Val PPL: 11.66\n",
      "[Step 800] Train Loss: 2.408, Val Loss: 2.428 | Train PPL: 11.11, Val PPL: 11.34\n",
      "[Step 900] Train Loss: 2.377, Val Loss: 2.371 | Train PPL: 10.77, Val PPL: 10.71\n",
      "[Step 1000] Train Loss: 2.352, Val Loss: 2.338 | Train PPL: 10.51, Val PPL: 10.36\n",
      "[Step 1100] Train Loss: 2.306, Val Loss: 2.326 | Train PPL: 10.04, Val PPL: 10.24\n",
      "[Step 1200] Train Loss: 2.295, Val Loss: 2.330 | Train PPL: 9.92, Val PPL: 10.28\n",
      "[Step 1300] Train Loss: 2.275, Val Loss: 2.295 | Train PPL: 9.73, Val PPL: 9.92\n",
      "[Step 1400] Train Loss: 2.221, Val Loss: 2.290 | Train PPL: 9.22, Val PPL: 9.87\n",
      "[Step 1500] Train Loss: 2.256, Val Loss: 2.269 | Train PPL: 9.55, Val PPL: 9.67\n",
      "[Step 1600] Train Loss: 2.228, Val Loss: 2.204 | Train PPL: 9.28, Val PPL: 9.06\n",
      "[Step 1700] Train Loss: 2.190, Val Loss: 2.190 | Train PPL: 8.94, Val PPL: 8.93\n",
      "[Step 1800] Train Loss: 2.223, Val Loss: 2.200 | Train PPL: 9.23, Val PPL: 9.03\n",
      "[Step 1900] Train Loss: 2.173, Val Loss: 2.168 | Train PPL: 8.78, Val PPL: 8.74\n",
      "[Step 2000] Train Loss: 2.180, Val Loss: 2.160 | Train PPL: 8.85, Val PPL: 8.67\n",
      "[Step 2100] Train Loss: 2.153, Val Loss: 2.171 | Train PPL: 8.61, Val PPL: 8.77\n",
      "[Step 2200] Train Loss: 2.118, Val Loss: 2.159 | Train PPL: 8.32, Val PPL: 8.66\n",
      "[Step 2300] Train Loss: 2.165, Val Loss: 2.138 | Train PPL: 8.72, Val PPL: 8.48\n",
      "[Step 2400] Train Loss: 2.121, Val Loss: 2.146 | Train PPL: 8.34, Val PPL: 8.55\n",
      "[Step 2500] Train Loss: 2.112, Val Loss: 2.137 | Train PPL: 8.26, Val PPL: 8.47\n",
      "[Step 2600] Train Loss: 2.158, Val Loss: 2.133 | Train PPL: 8.65, Val PPL: 8.44\n",
      "[Step 2700] Train Loss: 2.111, Val Loss: 2.099 | Train PPL: 8.26, Val PPL: 8.16\n",
      "[Step 2800] Train Loss: 2.091, Val Loss: 2.065 | Train PPL: 8.09, Val PPL: 7.89\n",
      "[Step 2900] Train Loss: 2.115, Val Loss: 2.091 | Train PPL: 8.29, Val PPL: 8.09\n",
      "[Step 3000] Train Loss: 2.120, Val Loss: 2.066 | Train PPL: 8.33, Val PPL: 7.89\n",
      "[Step 3100] Train Loss: 2.077, Val Loss: 2.109 | Train PPL: 7.98, Val PPL: 8.24\n",
      "[Step 3200] Train Loss: 2.067, Val Loss: 2.084 | Train PPL: 7.90, Val PPL: 8.03\n",
      "[Step 3300] Train Loss: 2.036, Val Loss: 2.043 | Train PPL: 7.66, Val PPL: 7.71\n",
      "[Step 3400] Train Loss: 2.066, Val Loss: 2.067 | Train PPL: 7.89, Val PPL: 7.90\n",
      "[Step 3500] Train Loss: 2.023, Val Loss: 2.067 | Train PPL: 7.56, Val PPL: 7.90\n",
      "[Step 3600] Train Loss: 2.043, Val Loss: 2.074 | Train PPL: 7.72, Val PPL: 7.96\n",
      "[Step 3700] Train Loss: 2.029, Val Loss: 2.083 | Train PPL: 7.61, Val PPL: 8.03\n",
      "[Step 3800] Train Loss: 2.027, Val Loss: 2.030 | Train PPL: 7.59, Val PPL: 7.61\n",
      "[Step 3900] Train Loss: 2.002, Val Loss: 2.049 | Train PPL: 7.41, Val PPL: 7.76\n",
      "[Step 4000] Train Loss: 2.006, Val Loss: 2.035 | Train PPL: 7.44, Val PPL: 7.65\n",
      "[Step 4100] Train Loss: 2.058, Val Loss: 1.987 | Train PPL: 7.83, Val PPL: 7.29\n",
      "[Step 4200] Train Loss: 2.066, Val Loss: 1.996 | Train PPL: 7.90, Val PPL: 7.36\n",
      "[Step 4300] Train Loss: 1.999, Val Loss: 2.021 | Train PPL: 7.38, Val PPL: 7.55\n",
      "[Step 4400] Train Loss: 2.024, Val Loss: 1.995 | Train PPL: 7.57, Val PPL: 7.35\n",
      "[Step 4500] Train Loss: 1.981, Val Loss: 2.039 | Train PPL: 7.25, Val PPL: 7.68\n",
      "[Step 4600] Train Loss: 1.979, Val Loss: 1.994 | Train PPL: 7.23, Val PPL: 7.35\n",
      "[Step 4700] Train Loss: 1.989, Val Loss: 1.998 | Train PPL: 7.31, Val PPL: 7.37\n",
      "[Step 4800] Train Loss: 1.988, Val Loss: 2.018 | Train PPL: 7.30, Val PPL: 7.52\n",
      "[Step 4900] Train Loss: 1.994, Val Loss: 2.003 | Train PPL: 7.35, Val PPL: 7.41\n",
      "[Step 5000] Train Loss: 1.972, Val Loss: 1.986 | Train PPL: 7.18, Val PPL: 7.29\n",
      "Checkpoint saved at iteration 5000\n",
      "[Step 5100] Train Loss: 1.964, Val Loss: 1.968 | Train PPL: 7.12, Val PPL: 7.15\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        train_loss, train_ppl = losses['train']\n",
    "        val_loss, val_ppl = losses['val']\n",
    "        print(f\"[Step {iter}] Train Loss: {train_loss:.3f}, Val Loss: {val_loss:.3f} | Train PPL: {train_ppl:.2f}, Val PPL: {val_ppl:.2f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save checkpoint every 5000 iterations\n",
    "    if iter % 5000 == 0 and iter > 0:\n",
    "        with open(f'model-04-{iter}.pkl', 'wb') as f:\n",
    "            pickle.dump(m, f)\n",
    "        print(f\"Checkpoint saved at iteration {iter}\")\n",
    "        torch.save({\n",
    "            'model-04': m.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'iter': iter\n",
    "        }, f'checkpoint_{iter}.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c395cb",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48c5b14",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mmodel-04.pkl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mpickle\u001b[49m.dump(model, f)\n",
      "\u001b[31mNameError\u001b[39m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "with open('model-04.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65560a",
   "metadata": {},
   "source": [
    "Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc39e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t. Looking gentle, “that was the dremble.\n",
      "Englation was the land subjected about\n",
      "cours a mad she felt to be to whole fire. In them she lead not damply and the brough of itstophing and passas chippear we had\n",
      "they one above the rightled of hone agate seemeding infell and everage will pan a losse\n",
      "wanter down ration her have a spipent it a candow.”\n",
      "I that is when the countaine. The prichman on the blookh ancous him this collding to his browere are where it chour, to save\n",
      "HEMES.\n",
      "resendentional docting\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82526f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
